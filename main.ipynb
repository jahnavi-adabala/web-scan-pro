{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdf5a99d-8318-4874-a95d-e0af5d6d7d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] HTML report created at Reports\\vulnerability_report.html\n",
      "Run complete. Summary: {'crawler': {'pages': 9, 'forms': 0}, 'sql': 0, 'xss': 0, 'auth': 3, 'session_fixation': 1, 'idor': 0}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse\n",
    "from jinja2 import Template\n",
    "\n",
    "DEFAULT_HEADERS = {\"User-Agent\": \"WebScanPro/1.0\"}\n",
    "SQL_ERROR_PATTERNS = [\n",
    "    r\"you have an error in your sql syntax\",\n",
    "    r\"warning: mysql_\",\n",
    "    r\"unclosed quotation mark\",\n",
    "    r\"quoted string not properly terminated\",\n",
    "    r\"pdoexception\",\n",
    "    r\"pg::syntaxerror\",\n",
    "    r\"mysql server version\",\n",
    "    r\"sqlite3::sqlexception\",\n",
    "    r\"sqlstate\"\n",
    "]\n",
    "XSS_REFLECT_PATTERNS = [\n",
    "    r\"<script>alert\\(\",\n",
    "    r\"onerror=\",\n",
    "    r\"<svg\",\n",
    "    r\"document\\.cookie\",\n",
    "    r\"innerHTML\\s*=\",\n",
    "    r\"eval\\(\"\n",
    "]\n",
    "\n",
    "AUTHORIZATION_CONFIRMED = False\n",
    "TARGET_BASE = \"http://127.0.0.1:8081/dvwa\"\n",
    "LOGIN_PATH = \"/login.php\"\n",
    "USERNAME = \"admin\"\n",
    "PASSWORD = \"password\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "REPORTS_DIR = \"Reports\"\n",
    "CRAWL_FILE = os.path.join(OUTPUT_DIR, \"crawler_output.json\")\n",
    "SQL_FILE = os.path.join(OUTPUT_DIR, \"sql_findings.json\")\n",
    "XSS_FILE = os.path.join(OUTPUT_DIR, \"xss_findings.json\")\n",
    "AUTH_FILE = os.path.join(OUTPUT_DIR, \"auth_findings.json\")\n",
    "IDOR_FILE = os.path.join(OUTPUT_DIR, \"idor_findings.json\")\n",
    "SUMMARY_FILE = os.path.join(OUTPUT_DIR, \"summary.json\")\n",
    "REPORT_HTML = os.path.join(REPORTS_DIR, \"vulnerability_report.html\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(REPORTS_DIR, exist_ok=True)\n",
    "\n",
    "def get_session():\n",
    "    s = requests.Session()\n",
    "    s.headers.update(DEFAULT_HEADERS)\n",
    "    return s\n",
    "\n",
    "session = get_session()\n",
    "\n",
    "def is_same_domain(base, url):\n",
    "    try:\n",
    "        return urlparse(base).netloc == urlparse(url).netloc\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def normalize_url(base, link):\n",
    "    if not link:\n",
    "        return None\n",
    "    return urljoin(base, link)\n",
    "\n",
    "def make_soup(html):\n",
    "    try:\n",
    "        return BeautifulSoup(html, \"lxml\")\n",
    "    except Exception:\n",
    "        return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "def extract_links(html, base_url):\n",
    "    soup = make_soup(html)\n",
    "    urls = set()\n",
    "    for tag in soup.find_all([\"a\",\"link\",\"area\"], href=True):\n",
    "        href = tag[\"href\"].strip()\n",
    "        u = normalize_url(base_url, href)\n",
    "        if u:\n",
    "            urls.add(u)\n",
    "    for f in soup.find_all(\"form\"):\n",
    "        action = f.get(\"action\") or base_url\n",
    "        u = normalize_url(base_url, action)\n",
    "        if u:\n",
    "            urls.add(u)\n",
    "    return urls\n",
    "\n",
    "def extract_forms(html, base_url):\n",
    "    soup = make_soup(html)\n",
    "    forms = []\n",
    "    for f in soup.find_all(\"form\"):\n",
    "        action = f.get(\"action\") or base_url\n",
    "        action = normalize_url(base_url, action)\n",
    "        method = (f.get(\"method\") or \"GET\").upper()\n",
    "        inputs = []\n",
    "        for inp in f.find_all([\"input\",\"textarea\",\"select\"]):\n",
    "            name = inp.get(\"name\")\n",
    "            typ = (inp.get(\"type\") or inp.name or \"\").lower()\n",
    "            val = inp.get(\"value\",\"\")\n",
    "            inputs.append({\"name\": name, \"type\": typ, \"value\": val})\n",
    "        forms.append({\"page\": base_url, \"action\": action, \"method\": method, \"inputs\": inputs})\n",
    "    return forms\n",
    "\n",
    "def dvwa_login(username=USERNAME, password=PASSWORD):\n",
    "    try:\n",
    "        r = session.post(urljoin(TARGET_BASE, LOGIN_PATH), data={\"username\": username, \"password\": password, \"Login\": \"Login\"}, timeout=6)\n",
    "        body = (r.text or \"\").lower()\n",
    "        if any(x in body for x in [\"logout\", \"security level\", \"dvwa\"]) or r.status_code in (302,303):\n",
    "            return True\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def crawl(start=TARGET_BASE, max_pages=200, delay=0.05):\n",
    "    visited = set()\n",
    "    pages = {}\n",
    "    forms = []\n",
    "    queue = [start]\n",
    "    while queue and len(visited) < max_pages:\n",
    "        url = queue.pop(0)\n",
    "        if url in visited:\n",
    "            continue\n",
    "        try:\n",
    "            r = session.get(url, timeout=6)\n",
    "            html = r.text or \"\"\n",
    "            links = list(extract_links(html, url))\n",
    "            page_forms = extract_forms(html, url)\n",
    "            pages[url] = {\"status\": r.status_code, \"links\": links, \"forms\": page_forms}\n",
    "            forms.extend(page_forms)\n",
    "            for L in links:\n",
    "                if L.startswith(TARGET_BASE) and L not in visited and L not in queue:\n",
    "                    queue.append(L)\n",
    "            visited.add(url)\n",
    "            time.sleep(delay)\n",
    "        except Exception:\n",
    "            visited.add(url)\n",
    "            time.sleep(delay)\n",
    "    result = {\"pages\": pages, \"forms\": forms}\n",
    "    with open(CRAWL_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "    return result\n",
    "\n",
    "SQL_PAYLOADS = [\"' OR '1'='1\", \"' OR 1=1--\", \"\\\" OR \\\"1\\\"=\\\"1\", \"'; DROP TABLE users; --\"]\n",
    "\n",
    "def find_sql_errors(text):\n",
    "    if not text:\n",
    "        return False, None\n",
    "    low = text.lower()\n",
    "    for pat in SQL_ERROR_PATTERNS:\n",
    "        if re.search(pat, low):\n",
    "            return True, pat\n",
    "    return False, None\n",
    "\n",
    "def test_url_params_for_sqli(urls, timeout=6, delay=0.1):\n",
    "    findings = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            params = parse_qs(parsed.query)\n",
    "            if not params:\n",
    "                continue\n",
    "            base = urlunparse((parsed.scheme, parsed.netloc, parsed.path, \"\", \"\", \"\"))\n",
    "            for param in params:\n",
    "                orig = params[param][0] if params[param] else \"\"\n",
    "                for payload in SQL_PAYLOADS:\n",
    "                    tp = dict(params)\n",
    "                    tp[param] = orig + payload\n",
    "                    q = urlencode(tp, doseq=True)\n",
    "                    test_url = base + \"?\" + q\n",
    "                    try:\n",
    "                        r = session.get(test_url, timeout=timeout)\n",
    "                        vuln, sig = find_sql_errors(r.text)\n",
    "                        if vuln:\n",
    "                            findings.append({\"type\": \"SQLi\", \"url\": test_url, \"param\": param, \"payload\": payload, \"error\": sig, \"status\": r.status_code})\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    time.sleep(delay)\n",
    "        except Exception:\n",
    "            pass\n",
    "    with open(SQL_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(findings, f, indent=2)\n",
    "    return findings\n",
    "\n",
    "XSS_PAYLOADS = ['\"><script>alert(1)</script>', \"<svg/onload=alert(1)>\", \"<img src=x onerror=alert(1)>\"]\n",
    "\n",
    "def detect_reflected_xss(text):\n",
    "    if not text:\n",
    "        return False, None\n",
    "    low = text.lower()\n",
    "    for pat in XSS_REFLECT_PATTERNS:\n",
    "        if re.search(pat, low):\n",
    "            return True, pat\n",
    "    return False, None\n",
    "\n",
    "def test_xss_in_urls(urls, timeout=6, delay=0.1):\n",
    "    findings = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            params = parse_qs(parsed.query)\n",
    "            if not params:\n",
    "                continue\n",
    "            base = urlunparse((parsed.scheme, parsed.netloc, parsed.path, \"\", \"\", \"\"))\n",
    "            for param in params:\n",
    "                for payload in XSS_PAYLOADS:\n",
    "                    tp = dict(params)\n",
    "                    tp[param] = payload\n",
    "                    q = urlencode(tp, doseq=True)\n",
    "                    test_url = base + \"?\" + q\n",
    "                    try:\n",
    "                        r = session.get(test_url, timeout=timeout)\n",
    "                        found, sig = detect_reflected_xss(r.text)\n",
    "                        if found or payload in (r.text or \"\"):\n",
    "                            findings.append({\"type\": \"XSS\", \"url\": test_url, \"param\": param, \"payload\": payload, \"status\": r.status_code})\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    time.sleep(delay)\n",
    "        except Exception:\n",
    "            pass\n",
    "    with open(XSS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(findings, f, indent=2)\n",
    "    return findings\n",
    "\n",
    "def test_xss_in_forms(forms, timeout=6, delay=0.1):\n",
    "    findings = []\n",
    "    for form in forms:\n",
    "        action = form.get(\"action\")\n",
    "        method = form.get(\"method\", \"GET\").upper()\n",
    "        inputs = [i for i in form.get(\"inputs\", []) if i.get(\"name\")]\n",
    "        if not inputs:\n",
    "            continue\n",
    "        for inp in inputs:\n",
    "            for payload in XSS_PAYLOADS:\n",
    "                data = {i[\"name\"]: (payload if i[\"name\"] == inp[\"name\"] else (i.get(\"value\") or \"test\")) for i in inputs}\n",
    "                try:\n",
    "                    if method == \"POST\":\n",
    "                        r = session.post(action, data=data, timeout=timeout)\n",
    "                    else:\n",
    "                        r = session.get(action, params=data, timeout=timeout)\n",
    "                    found, sig = detect_reflected_xss(r.text)\n",
    "                    if found or payload in (r.text or \"\"):\n",
    "                        findings.append({\"type\": \"XSS\", \"action\": action, \"field\": inp[\"name\"], \"payload\": payload, \"status\": r.status_code})\n",
    "                except Exception:\n",
    "                    pass\n",
    "                time.sleep(delay)\n",
    "    existing = []\n",
    "    try:\n",
    "        with open(XSS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            existing = json.load(f)\n",
    "    except Exception:\n",
    "        existing = []\n",
    "    existing.extend(findings)\n",
    "    with open(XSS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(existing, f, indent=2)\n",
    "    return existing\n",
    "\n",
    "WEAK_CREDS = [{\"username\": \"admin\", \"password\": \"password\"}, {\"username\": \"admin\", \"password\": \"admin\"}, {\"username\": \"user\", \"password\": \"user\"}]\n",
    "\n",
    "def test_weak_credentials(login_path=LOGIN_PATH, attempts=WEAK_CREDS, timeout=6, delay=0.2):\n",
    "    results = []\n",
    "    for cred in attempts:\n",
    "        try:\n",
    "            s = requests.Session()\n",
    "            r = s.post(urljoin(TARGET_BASE, login_path), data={\"username\": cred[\"username\"], \"password\": cred[\"password\"], \"Login\": \"Login\"}, timeout=timeout)\n",
    "            body = (r.text or \"\").lower()\n",
    "            success = any(x in body for x in [\"logout\", \"welcome\", \"dashboard\"]) or r.status_code in (302,303)\n",
    "            results.append({\"username\": cred[\"username\"], \"password\": cred[\"password\"], \"success\": bool(success), \"status\": r.status_code})\n",
    "        except Exception as e:\n",
    "            results.append({\"username\": cred[\"username\"], \"password\": cred[\"password\"], \"success\": False, \"error\": str(e)})\n",
    "        time.sleep(delay)\n",
    "    with open(AUTH_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    return results\n",
    "\n",
    "def test_session_fixation(acct, timeout=6):\n",
    "    findings = []\n",
    "    try:\n",
    "        s_att = requests.Session()\n",
    "        s_vic = requests.Session()\n",
    "        domain = urlparse(TARGET_BASE).hostname\n",
    "        fixed_val = \"fixedsessiontest123\"\n",
    "        s_att.cookies.set(\"sessionid\", fixed_val, domain=domain, path=\"/\")\n",
    "        s_vic.cookies.set(\"sessionid\", fixed_val, domain=domain, path=\"/\")\n",
    "        s_vic.post(urljoin(TARGET_BASE, LOGIN_PATH), data={\"username\": acct[\"username\"], \"password\": acct[\"password\"], \"Login\": \"Login\"}, timeout=timeout)\n",
    "        vic_cookies = {c.name: c.value for c in s_vic.cookies}\n",
    "        for name, val in vic_cookies.items():\n",
    "            if val == fixed_val:\n",
    "                findings.append({\"type\": \"session_fixation\", \"cookie\": name, \"detail\": \"cookie unchanged after login\"})\n",
    "                break\n",
    "    except Exception:\n",
    "        pass\n",
    "    return findings\n",
    "\n",
    "ID_PARAM_HINTS = re.compile(r\"(?:^|_|\\b)(id|user|order|invoice|file|doc|profile|acct|account)(?:$|\\b)\", re.I)\n",
    "\n",
    "def is_uuid_like(s):\n",
    "    return bool(re.match(r\"^[0-9a-fA-F\\-]{8,36}$\", s))\n",
    "\n",
    "def collect_candidate_from_url(u):\n",
    "    candidates = []\n",
    "    parsed = urlparse(u)\n",
    "    qs = parse_qs(parsed.query)\n",
    "    for p, vals in qs.items():\n",
    "        if ID_PARAM_HINTS.search(p) or any(v.isdigit() or is_uuid_like(v) for v in vals):\n",
    "            candidates.append({\"type\": \"query\", \"url\": u, \"param\": p, \"value\": vals[0] if vals else \"\", \"method\": \"GET\"})\n",
    "    segs = [s for s in parsed.path.split(\"/\") if s]\n",
    "    for i, seg in enumerate(segs):\n",
    "        if seg.isdigit() or is_uuid_like(seg):\n",
    "            candidates.append({\"type\": \"path\", \"url\": u, \"path_index\": i, \"value\": seg, \"method\": \"GET\"})\n",
    "    return candidates\n",
    "\n",
    "def collect_candidate_from_form(f):\n",
    "    cands = []\n",
    "    action = f.get(\"action\") or f.get(\"page\")\n",
    "    method = f.get(\"method\", \"GET\").upper()\n",
    "    for inp in f.get(\"inputs\", []):\n",
    "        name = inp.get(\"name\")\n",
    "        if not name:\n",
    "            continue\n",
    "    if ID_PARAM_HINTS.search(name) or (inp.get(\"type\") and inp.get(\"type\").lower() in (\"hidden\", \"number\")):\n",
    "        cands.append({\"type\": \"form\", \"url\": action, \"param\": name, \"method\": method, \"value\": inp.get(\"value\", \"\")})\n",
    "    return cands\n",
    "\n",
    "def discover_candidates(crawl):\n",
    "    pages = crawl.get(\"pages\", {})\n",
    "    forms = crawl.get(\"forms\", [])\n",
    "    candidates = []\n",
    "    for u in pages.keys():\n",
    "        candidates.extend(collect_candidate_from_url(u))\n",
    "    for f in forms:\n",
    "        candidates.extend(collect_candidate_from_form(f))\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for c in candidates:\n",
    "        key = (c.get(\"url\"), c.get(\"param\", \"\"), c.get(\"type\"), str(c.get(\"path_index\", \"\")))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        unique.append(c)\n",
    "    return unique\n",
    "\n",
    "def active_check_idor_query(c, acct_from, max_probes=5, delay=0.6):\n",
    "    findings = []\n",
    "    parsed = urlparse(c[\"url\"])\n",
    "    qs = parse_qs(parsed.query)\n",
    "    original = qs.get(c[\"param\"], [None])[0]\n",
    "    if original is None:\n",
    "        return findings\n",
    "    repls = []\n",
    "    if original.isdigit():\n",
    "        v = int(original)\n",
    "        for d in range(1, max_probes + 1):\n",
    "            repls.append(str(v + d))\n",
    "            if v - d > 0:\n",
    "                repls.append(str(v - d))\n",
    "    repls.extend([str(i) for i in range(1, min(5, max_probes + 1))])\n",
    "    repls = list(dict.fromkeys(repls))[:max_probes]\n",
    "    s = requests.Session()\n",
    "    try:\n",
    "        s.post(urljoin(TARGET_BASE, LOGIN_PATH), data={\"username\": acct_from[\"username\"], \"password\": acct_from[\"password\"], \"Login\": \"Login\"}, timeout=6)\n",
    "    except Exception:\n",
    "        pass\n",
    "    for rpl in repls:\n",
    "        try:\n",
    "            qs2 = dict(qs)\n",
    "            qs2[c[\"param\"]] = rpl\n",
    "            newq = urlencode(qs2, doseq=True)\n",
    "            newurl = urlunparse(parsed._replace(query=newq))\n",
    "            resp = s.get(newurl, timeout=6)\n",
    "            findings.append({\"type\": \"IDOR_probe\", \"as_user\": acct_from[\"username\"], \"tested_value\": rpl, \"url\": newurl, \"status\": resp.status_code, \"length\": len(resp.text or \"\")})\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(delay)\n",
    "    return findings\n",
    "\n",
    "def classify_for_report(item):\n",
    "    t = (item.get(\"type\") or \"\").lower() if isinstance(item, dict) else \"\"\n",
    "    if \"sqli\" in t or \"sql\" in t or \"id\" in t or \"idor\" in t or \"session\" in t:\n",
    "        return \"High\"\n",
    "    if \"xss\" in t or \"cookie\" in t or \"csrf\" in t or \"token\" in t:\n",
    "        return \"Medium\"\n",
    "    return \"Low\"\n",
    "\n",
    "def embed_bar_chart(data_counts):\n",
    "    labels = list(data_counts.keys())\n",
    "    values = [data_counts.get(k, 0) for k in labels]\n",
    "    fig = plt.figure(figsize=(6,3))\n",
    "    plt.bar(labels, values, color=[\"#b00020\", \"#d97706\", \"#107b10\"])\n",
    "    plt.title(\"Vulnerabilities by Severity\")\n",
    "    plt.tight_layout()\n",
    "    buf = BytesIO()\n",
    "    fig.savefig(buf, format=\"png\")\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "    b64 = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "    buf.close()\n",
    "    return \"data:image/png;base64,\" + b64\n",
    "\n",
    "def generate_report(crawl_result, sql_find, xss_find, auth_find, idor_find):\n",
    "    findings = []\n",
    "    for i in sql_find:\n",
    "        findings.append({\"type\": \"SQLi\", \"endpoint\": i.get(\"url\") or i.get(\"endpoint\", \"N/A\"), \"severity\": classify_for_report(i), \"mitigation\": \"Use parameterized queries / prepared statements\", \"evidence\": i.get(\"error\") or \"\"})\n",
    "    for i in xss_find:\n",
    "        findings.append({\"type\": \"XSS\", \"endpoint\": i.get(\"url\") or i.get(\"action\") or \"N/A\", \"severity\": classify_for_report(i), \"mitigation\": \"Sanitize and encode output, use CSP\", \"evidence\": i.get(\"payload\") or i.get(\"field\") or \"\"})\n",
    "    for i in auth_find:\n",
    "        findings.append({\"type\": \"Weak Credential\", \"endpoint\": urljoin(TARGET_BASE, LOGIN_PATH), \"severity\": classify_for_report(i), \"mitigation\": \"Enforce strong passwords and rate limiting\", \"evidence\": str(i)})\n",
    "    for i in idor_find:\n",
    "        findings.append({\"type\": \"IDOR\", \"endpoint\": i.get(\"url\") or i.get(\"tested_url\", \"N/A\"), \"severity\": classify_for_report(i), \"mitigation\": \"Enforce server-side ownership checks (RBAC/ABAC)\", \"evidence\": i.get(\"tested_value\") or \"\"})\n",
    "    sev_counts = {\"High\": 0, \"Medium\": 0, \"Low\": 0}\n",
    "    for f in findings:\n",
    "        sev_counts[f[\"severity\"]] = sev_counts.get(f[\"severity\"], 0) + 1\n",
    "    img_src = embed_bar_chart(sev_counts)\n",
    "    tpl = Template(\"\"\"<!doctype html>\n",
    "<html>\n",
    "<head>\n",
    "<meta charset=\"utf-8\">\n",
    "<title>Vulnerability Report</title>\n",
    "<style>\n",
    "body{font-family:Arial,Helvetica,sans-serif;margin:24px;color:#222}\n",
    "h1{margin-bottom:6px}\n",
    ".summary{margin-bottom:18px}\n",
    ".table{border-collapse:collapse;width:100%;margin-top:18px}\n",
    ".table th,.table td{border:1px solid #ddd;padding:8px;text-align:left}\n",
    ".table th{background:#f4f4f4}\n",
    ".sev-High{color:#b00020;font-weight:700}\n",
    ".sev-Medium{color:#d97706;font-weight:700}\n",
    ".sev-Low{color:#107b10;font-weight:700}\n",
    ".small{font-size:0.9em;color:#555}\n",
    "pre{white-space:pre-wrap;font-family:inherit}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<h1>Vulnerability Assessment Report</h1>\n",
    "<div class=\"summary\">\n",
    "  <div style=\"display:flex;gap:20px;align-items:center\">\n",
    "    <div><img src=\"{{ img }}\" alt=\"chart\" width=\"420\"></div>\n",
    "    <div>\n",
    "      <div class=\"small\">Target: {{ target }}</div>\n",
    "      <div class=\"small\">Total findings: {{ total }}</div>\n",
    "      <ul>\n",
    "        <li class=\"sev-High\">High: {{ counts['High'] }}</li>\n",
    "        <li class=\"sev-Medium\">Medium: {{ counts['Medium'] }}</li>\n",
    "        <li class=\"sev-Low\">Low: {{ counts['Low'] }}</li>\n",
    "      </ul>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "<table class=\"table\">\n",
    "<thead><tr><th>#</th><th>Vulnerability</th><th>Affected endpoint</th><th>Severity</th><th>Suggested mitigation</th><th>Evidence</th></tr></thead>\n",
    "<tbody>\n",
    "{% for f in findings %}\n",
    "<tr>\n",
    "<td>{{ loop.index }}</td>\n",
    "<td>{{ f.type }}</td>\n",
    "<td>{{ f.endpoint }}</td>\n",
    "<td class=\"sev-{{ f.severity }}\">{{ f.severity }}</td>\n",
    "<td>{{ f.mitigation }}</td>\n",
    "<td><pre>{{ f.evidence }}</pre></td>\n",
    "</tr>\n",
    "{% endfor %}\n",
    "</tbody>\n",
    "</table>\n",
    "</body>\n",
    "</html>\"\"\")\n",
    "    html = tpl.render(findings=findings, counts=sev_counts, total=len(findings), img=img_src, target=TARGET_BASE)\n",
    "    with open(REPORT_HTML, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "    with open(os.path.join(OUTPUT_DIR, \"report_data.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"findings\": findings, \"counts\": sev_counts}, f, indent=2)\n",
    "    print(\"[+] HTML report created at\", REPORT_HTML)\n",
    "\n",
    "def run_all():\n",
    "    dvwa_login()\n",
    "    crawl_result = crawl()\n",
    "    urls = list(crawl_result.get(\"pages\", {}).keys())\n",
    "    sql_find = test_url_params_for_sqli(urls)\n",
    "    xss_find_urls = test_xss_in_urls(urls)\n",
    "    xss_find_forms = test_xss_in_forms(crawl_result.get(\"forms\", []))\n",
    "    xss_find = xss_find_urls\n",
    "    auth_find = test_weak_credentials()\n",
    "    sf = test_session_fixation({\"username\": USERNAME, \"password\": PASSWORD})\n",
    "    idor_candidates = discover_candidates(crawl_result)\n",
    "    idor_findings = []\n",
    "    if AUTHORIZATION_CONFIRMED:\n",
    "        test_accounts = [{\"username\": USERNAME, \"password\": PASSWORD}]\n",
    "        for i in range(len(test_accounts)):\n",
    "            for j in range(len(test_accounts)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                for c in idor_candidates:\n",
    "                    if c[\"type\"] == \"query\":\n",
    "                        idor_findings.extend(active_check_idor_query(c, test_accounts[i]))\n",
    "        if test_accounts:\n",
    "            idor_findings.extend(test_session_fixation(test_accounts[0]))\n",
    "    with open(SQL_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sql_find, f, indent=2)\n",
    "    with open(XSS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(xss_find, f, indent=2)\n",
    "    with open(AUTH_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(auth_find, f, indent=2)\n",
    "    with open(IDOR_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(idor_findings, f, indent=2)\n",
    "    summary = {\"crawler\": {\"pages\": len(crawl_result.get(\"pages\", {})), \"forms\": len(crawl_result.get(\"forms\", []))}, \"sql\": len(sql_find), \"xss\": len(xss_find), \"auth\": len(auth_find), \"session_fixation\": len(sf), \"idor\": len(idor_findings)}\n",
    "    with open(SUMMARY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    generate_report(crawl_result, sql_find, xss_find, auth_find, idor_findings)\n",
    "    print(\"Run complete. Summary:\", summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b321a0-07e5-44a9-8d80-68215d3d2f47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
