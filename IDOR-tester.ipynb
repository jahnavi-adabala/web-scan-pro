{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd778b-c394-494d-9f46-7a701fc09dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Starting DVWA IDOR tester\n",
      "[+] Login appears successful.\n",
      "[crawl] http://localhost/setup.php -> 200 ; links=3 forms=1\n",
      "[+] Crawl complete. 1 pages saved to dvwa_crawl.json\n",
      "[+] Discovered 1 candidate(s) for IDOR testing\n",
      "  [passive] form user_token -> 200 (4075 bytes)\n",
      "[*] Active checks skipped. Set AUTHORIZATION_CONFIRMED=True to enable active tests (ONLY in authorized lab).\n",
      "[*] Done. Findings saved to week6_access_control_findings.json (2 items).\n",
      "Sample: {'type': 'passive_candidate_observed', 'candidate': {'type': 'form', 'url': 'http://localhost/setup.php', 'param': 'user_token', 'method': 'POST', 'value': '5fc1fbe3eed0b30fa1c8e258c904d1ce'}, 'status': 200, 'length': 4075}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re, time, json, requests\n",
    "from urllib.parse import urlparse, urljoin, urlunparse, parse_qs, urlencode\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "AUTHORIZATION_CONFIRMED = False     \n",
    "TARGET_BASE = \"http://localhost/setup.php\"\n",
    "LOGIN_PATH = \"/login.php\"\n",
    "USERNAME = \"admin\"\n",
    "PASSWORD = \"password\"\n",
    "CRAWL_SAVE = \"dvwa_crawl.json\"\n",
    "FINDINGS_SAVE = \"week6_access_control_findings.json\"\n",
    "\n",
    "MAX_PAGES = 40\n",
    "REQUEST_TIMEOUT = 6\n",
    "PASSIVE_DELAY = 0.05\n",
    "ACTIVE_DELAY = 0.5\n",
    "MAX_ID_PROBES_PER_ENDPOINT = 5\n",
    "ID_PARAM_HINTS = re.compile(r\"(?:^|_|\\b)(id|user|order|invoice|file|doc|profile|acct|account)(?:$|\\b)\", re.I)\n",
    "\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\":\"Week6-IDOR-Tester/1.0\"})\n",
    "findings = []\n",
    "\n",
    "\n",
    "try:\n",
    "        return BeautifulSoup(html, \"lxml\")\n",
    "         except Exception:\n",
    "          return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "def full_target(path):\n",
    "    return urljoin(TARGET_BASE, path)\n",
    "\n",
    "def save_json(path, obj):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "def is_uuid_like(s):\n",
    "    return bool(re.match(r\"^[0-9a-fA-F\\-]{8,36}$\", s))\n",
    "\n",
    "\n",
    "def dvwa_login(username=USERNAME, password=PASSWORD):\n",
    "    login_url = full_target(LOGIN_PATH)\n",
    "    payload = {\"username\": username, \"password\": password, \"Login\": \"Login\"}\n",
    "    try:\n",
    "        r = session.post(login_url, data=payload, timeout=REQUEST_TIMEOUT)\n",
    "   \n",
    "        body = (r.text or \"\").lower()\n",
    "        if any(x in body for x in [\"logout\", \"dvwa\", \"security level\"] ) or r.status_code in (302,303):\n",
    "            print(\"[+] Login appears successful.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"[!] Login response received but success not clearly detected. Check manually.\")\n",
    "            return True  # still return True so crawler can run; you can adjust logic\n",
    "    except Exception as e:\n",
    "        print(\"[!] Login failed:\", e)\n",
    "        return False\n",
    "\n",
    "\n",
    "def extract_links(html, base_url):\n",
    "    soup = make_soup(html)\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "        if href.startswith(\"javascript:\") or href.startswith(\"#\"):\n",
    "            continue\n",
    "        absolute = urljoin(base_url, href)\n",
    "        links.append(absolute)\n",
    "    return links\n",
    "\n",
    "def extract_forms(html, page_url):\n",
    "    soup = make_soup(html)\n",
    "    forms = []\n",
    "    for f in soup.find_all(\"form\"):\n",
    "        action = f.get(\"action\") or page_url\n",
    "        action = urljoin(page_url, action)\n",
    "        method = (f.get(\"method\") or \"GET\").upper()\n",
    "        inputs = []\n",
    "        for inp in f.find_all([\"input\",\"textarea\",\"select\"]):\n",
    "            name = inp.get(\"name\")\n",
    "            typ = (inp.get(\"type\") or inp.name or \"\").lower()\n",
    "            value = inp.get(\"value\", \"\")\n",
    "            inputs.append({\"name\": name, \"type\": typ, \"value\": value})\n",
    "        forms.append({\"page\": page_url, \"action\": action, \"method\": method, \"inputs\": inputs})\n",
    "    return forms\n",
    "\n",
    "def crawl(start=TARGET_BASE, max_pages=MAX_PAGES, delay=PASSIVE_DELAY):\n",
    "    visited = set()\n",
    "    pages = {}\n",
    "    forms = []\n",
    "    q = deque([start])\n",
    "    while q and len(visited) < max_pages:\n",
    "        url = q.popleft()\n",
    "        if url in visited:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c0fe27-ce04-4fbf-ab62-16e7f3fd14e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
